# Daily Tracking
_14 July 2022_

## Originally Planned

### Swagit Scraper for Dataset Research
Write the swagit scraper for getting a basic understanding of which
and how much data is available from the Swagit domain.

### Software Search
Work on https://github.com/PugetSoundClinic-PIT/soft-search/issues/1 and
https://github.com/PugetSoundClinic-PIT/soft-search/issues/2 -- creating
an early prototype dataset for understanding how many proposals promise
the delivery of software.

### Read Speakerbox Papers
If I have extra time, read a speakerbox paper or two.

## Actually Worked On

### Swagit Scraper for Dataset Research
Spent the whole day on this. It shouldn't have taken as long admittedly but numerous
perfectionism streaks hit me. First was just setting up the repo I was frustrated that
my cookiecutter template was so out-of-date so I remade that. That isn't the
best use of my time and I recognize that (but a part of me says, "yay starting projects
will be easier in the future -- and it will be a good thing to show the undergrads").

After that I finally got started on the actual scraper and quickly thought about all the
things that could go wrong. Specifically, we want to scrape ~200,000 pages from a single
domain. I don't want to crash their site / we need to build in exponential backoffs and
such.

In addition, if the scraper dies at a certain point, we want to write out the results in
batches. All standard stuff, just increases complexity.

At one point I even tried to handle some "error pages" with selenium but after spending
thirty minutes trying to get it to work but couldn't, I skipped it.

Well, that is all done now. All of it. And it's nicely packaged too!

I am trying to test it on a sample of ~10,000 (100 chunks) and we are already being rate
limited so there is definitely some fine tuning to do.

Or maybe we randomly sample indices?